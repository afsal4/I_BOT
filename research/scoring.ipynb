{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring system for Interview Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for returnign the score based on the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(question, answer, job_description):\n",
    "    \"\"\"\n",
    "    7 or above 7 answered correctly\n",
    "    5 or above 5 answered but need to improve \n",
    "    3 or above 3 some knowledge about it but dont know the answer\n",
    "    0 or above 0 dont know the answer\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    llm = OpenAI(temperature=.5)\n",
    "\n",
    "    \n",
    "    template_2 =  \"\"\"\n",
    "    you are an scoring partner for interview questions for a company:\n",
    "    \n",
    "    the question that is asked would and the answer are given \n",
    "\n",
    "    you should score this out of 10:\n",
    "    scores should be between 1 to 10\n",
    "    7 or above 7 answered correctly\n",
    "    5 or above 5 answered but need to improve \n",
    "    3 or above 3 some knowledge about it but dont know the answer\n",
    "    0 or above 0 dont know the answer\n",
    "\n",
    "    \n",
    "    score based on: \n",
    "    * creativity \n",
    "    * honesty\n",
    "    * accuracy of answers \n",
    "    * manners\n",
    "    * indepth knowledge \n",
    "    * projects\n",
    "    * if the question doesnt have any importance answers doesnt matter so give points for that\n",
    "    \n",
    "    should reduce the points:\n",
    "    * unappropriate behaviour \n",
    "    * dont know the answers\n",
    "    * not giving proper answer\n",
    "    * not speaking proper english\n",
    "    \n",
    "    \n",
    "    Company Job description: {job_description}\n",
    "    \n",
    "    Question: {question}\n",
    "\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Company Job description is the criteria given by the company for hiring candidates\n",
    "    \n",
    "    Question is the question asked by the interviewer\n",
    "    \n",
    "    Answer is the answer given by the candidate\n",
    "    \n",
    "    Only return the score no need of any description should only answer in digits and nothing else\n",
    "    Only in this format and nothing else:\n",
    "    \n",
    "    Score: ..\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # template for scoring\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['question', 'answer', 'job_description'],\n",
    "        template=template_2\n",
    "    )\n",
    "    \n",
    "    # chaining the model and the template \n",
    "    chain = LLMChain(llm=llm, prompt=prompt, output_key='score')\n",
    "    \n",
    "    # response from the llm model \n",
    "    response = chain.invoke({'question': question, 'job_description': job_description, 'answer': answer})\n",
    "    \n",
    "    # cleaning the score output from llm\n",
    "    score = response['score'].strip()\n",
    "    score = [i for i in score if i.isdigit()][0]\n",
    "    \n",
    "    return int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "7\n",
      "9\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "question = \"what is machine learning\"\n",
    "answer = 'machine learning is a technique used for predicting data from the data that it has trained. it is used in object detection, stock prediction and etc'\n",
    "job_description = 'we are looking for a machine learning fresher who has indepth knowledge in python'\n",
    "\n",
    "for i in range(10):\n",
    "    print(scoring(question, answer, job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vivi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
